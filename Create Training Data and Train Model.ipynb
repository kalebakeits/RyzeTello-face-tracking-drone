{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file contains the code to:\n",
    "1. Prepare data for the model\n",
    "2. Define and train the model\n",
    "3. Save and test the model\n",
    "\n",
    "\n",
    "#### This file requires the following directories to be set up in advance:\n",
    "\n",
    "1. Faces/1/\n",
    "\n",
    "    ->Contains 1000+ images of the face you want to track\n",
    "    \n",
    "2. Faces/2/ \n",
    "\n",
    "    ->Contains 1000+ images of unwanted faces while tracking\n",
    "    \n",
    "3. Faces/1 Test/\n",
    "\n",
    "    ->Test images of the face to track\n",
    "    \n",
    "4. Faces/2 Test/ \n",
    "\n",
    "    ->Test images of unwanted faces\n",
    "\n",
    "*These directories aren't included as they contain sensitive data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = os.path.join(os.getcwd(),'Faces')\n",
    "CATEGORIES = [\"1\",\"0\"]\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DATADIR, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "        img_array = cv2.resize(img_array, (100,100))\n",
    "        plt.imshow(img_array, cmap = \"gray\")\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 100\n",
    "\n",
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    \"\"\"\n",
    "    Populate training_data with [image_array,label] \n",
    "    \"\"\"\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        label = CATEGORIES.index(category)\n",
    "        for img in tqdm(os.listdir(path)):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE))\n",
    "                training_data.append([new_array,label])\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:04<00:00, 242.85it/s]\n",
      "100%|██████████| 1012/1012 [00:01<00:00, 542.66it/s]\n"
     ]
    }
   ],
   "source": [
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2110\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "pickle_out = open('X.pickle','wb')\n",
    "pickle.dump(X,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('y.pickle','wb')\n",
    "pickle.dump(y,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation, training and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60/60 [==============================] - 40s 645ms/step - loss: 0.4695 - accuracy: 0.7948 - val_loss: 0.0687 - val_accuracy: 0.9668\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - 28s 473ms/step - loss: 0.0614 - accuracy: 0.9761 - val_loss: 0.0310 - val_accuracy: 0.9905\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - 30s 494ms/step - loss: 0.0385 - accuracy: 0.9902 - val_loss: 0.0323 - val_accuracy: 0.9810\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - 29s 482ms/step - loss: 0.0349 - accuracy: 0.9852 - val_loss: 0.0193 - val_accuracy: 0.9953\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - 29s 482ms/step - loss: 0.0141 - accuracy: 0.9962 - val_loss: 0.0077 - val_accuracy: 0.9953\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - 28s 467ms/step - loss: 0.0074 - accuracy: 0.9972 - val_loss: 0.0210 - val_accuracy: 0.9905\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - 28s 472ms/step - loss: 0.0036 - accuracy: 0.9999 - val_loss: 0.0157 - val_accuracy: 0.9905\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - 28s 465ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0078 - val_accuracy: 0.9953\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - 28s 471ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 0.9953\n",
      "Epoch 10/10\n",
      "60/60 [==============================] - 29s 481ms/step - loss: 6.2196e-04 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 0.9953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd6fb53a430>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pickle.load(open('X.pickle', 'rb'))\n",
    "y = np.array(pickle.load(open('y.pickle', 'rb')))\n",
    "X = X/255.0\n",
    "\n",
    "#Tensorflow Conv2D Sequential Model for image classification\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(3,3), input_shape = X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(X, y, batch_size = 32, validation_split = 0.1, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Face-Recognition.model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Face-Recognition.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "Separate files:\n",
    "1. 1 Test = Face you want to detect\n",
    "2. 2 Test = Faces you would ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50489295]] You\n",
      "[[0.3065823]] You\n",
      "[[0.7550906]] You\n",
      "[[0.5193197]] You\n",
      "[[0.11118051]] You\n",
      "[[0.23189941]] You\n",
      "[[0.5050461]] You\n",
      "[[0.9251879]] You\n",
      "[[0.5583346]] You\n",
      "[[0.6208454]] You\n",
      "[[0.5095783]] You\n",
      "[[0.3886503]] You\n",
      "[[0.6720265]] You\n",
      "[[1.]] Not You\n",
      "[[0.99733186]] Not You\n",
      "[[0.9999994]] Not You\n",
      "[[1.]] Not You\n",
      "[[0.99999976]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[0.999994]] Not You\n",
      "[[1.]] Not You\n",
      "[[0.98007053]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[1.]] Not You\n",
      "[[0.9999964]] Not You\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES =['You','Not You']\n",
    "names = [\"1 Test\", \"2 Test\"]\n",
    "pathname = \"Faces\"\n",
    "\n",
    "filepaths = []\n",
    "\n",
    "for name in names:\n",
    "    path = os.path.join(pathname,name)\n",
    "    for file in os.listdir(path):\n",
    "        filepath = os.path.join(path,file)\n",
    "        filepaths.append([filepath,CATEGORIES[names.index(name)]])\n",
    "    \n",
    "def prepare(filepath):\n",
    "    IMG_SIZE = 100\n",
    "    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE))/255.0\n",
    "    return  (np.array(new_array).reshape(-1,IMG_SIZE,IMG_SIZE,1).astype('float32'),img_array)\n",
    "\n",
    "model = tf.keras.models.load_model(\"Face-Recognition.model\")\n",
    "\n",
    "for file in filepaths:\n",
    "    prepare_out = prepare(file[0])\n",
    "    prediction = model.predict(prepare_out[0])\n",
    "    print(prediction, file[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
